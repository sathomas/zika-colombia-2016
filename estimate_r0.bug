# This is a JAGS model for estimating the basic reproduction
# number (R0) for Colombia departments. It relies on a
# hierarchical linear regression with two levels of
# hierarchy. The input data consists of a set of
# observations where each observation includes an
# independent variable (x) representing the week number, a
# dependent variable (y) representing the cumulative
# confirmed cases, and an integer-valued department
# identification.
#
# The model assumes that the dependent variable y is
# exponentially related to the independent variable, and
# observations within a department are more closely related
# than observations between departments. In particular, it
# assumes that the slope parameters for individual groups
# are normally distributed around an overall slope.
# Intercept parameters, however, are assumed independent for
# each group and are derived from the data.
#
# The model reports goodness of fit using Bayesian p-value.
#
# Observation data is supplied in three vectors:
#    dept[]  - department id
#    x[]     - week number
#    y[]     - cumulative confirmed cases
#
# In addition, a vector of observed intercepts for
# each deptartment is supplied in the `intercept[]` vector.

data {

     # Transform Inputs
     # ----------------

     # Logarithmic transformation to allow linear regression

     ln_y <- log(y)
}

model {

    # Prior Probabilities
    # -------------------
    # Express all priors as reference (non-informative)
    # priors. The parameters for the model are:
    #
    #   beta_mu     - the mean for the aggregate slope
    #                 (all depts)
    #   beta_sigma  - the standard deviation for the
    #                 aggregate slope (all depts)
    #   sigma       - the standard deviation for the
    #                 residuals (assumed to the same
    #                 for all depts)

    beta_mu     ~ dnorm(0, 0.0001)
    beta_sigma  ~ dunif(0.001, 1000)
    sigma       ~ dunif(0.001, 1000)

    # The Likelihood
    # --------------
    # Model the dependent variable `y` as Normal with mean
    # `mu` and standard deviation `sigma`. Model the mean
    # `mu` as a linear function of the independent variable
    # `x`. The parameters of that linear relationship vary
    # based on the deptartment, and are:
    #
    #   alpha[dept[i]] - intercept for department
    #   beta[dept[i]]  - slope for the department
    #
    # The standard deviation `sigma` is assumed to be the
    # same for all observations, regardless of dept.
    #
    # Apply the model to all observations.
    for (i in 1:length(y)) {
        ln_y[i] ~ dnorm(mu[i], 1/(sigma^2))
        mu[i]  <- alpha[dept[i]] + beta[dept[i]]*x[i]
    }

    # Model the slope for each dept as Normal with a mean
    # and standard deviation from hyper-priors. Apply the
    # model to all depts.
    for (j in 1:max(dept[])) {
        beta[j]  ~ dnorm(beta_mu,  1/(beta_sigma^2))
    }

    # The intercept values for each dept are included in
    # the observations. Use the observed value as the
    # mean for a Normal distribution with a standard
    # deviation equal to the overall standard deviation.
    for (j in 1:max(dept[])) {
        alpha[j] ~ dnorm(intercept[j], 1/(sigma^2))
    }

    # Posterior Predictive Check
    # --------------------------
    # Calculate the p-value using a sum-of-squares
    # test for fitness. To do that, first calculate
    # the sum of squares of the residuals. (The
    # residuals are the differences between the observed
    # y-values and the values that the current iteration's
    # parameters would predict.)
    for (i in 1:length(y)) {
        sq.res[i] <- pow((ln_y[i] - mu[i]), 2)
    }

    # Next, generate a new y-value given the current
    # iteration's parameters and compare that new
    # value with what the parameters would predict.
    for (i in 1:length(y)) {
        ln_y.new[i] ~ dnorm(mu.new[i], 1/(sigma^2))
        mu.new[i]  <- alpha[dept[i]] + beta[dept[i]]*x[i]
        sq.new[i]  <- pow((ln_y.new[i] - mu[i]),  2)
    }

    # For the goodness of fit test, compare the sums
    # of the two squared values to see which is greater.
    # If the model is a good fit, neither sum is more
    # likely to be greater than the the other. The
    # resulting p-value should, therefore, be close to
    # 0.5.
    test   <- step(sum(sq.new[]) - sum(sq.res[]))
    pvalue <- mean(test)

    # Additional Processing
    # ---------------------
    # Although not part of the regression itself, the
    # model can be used to estimate R0 from the
    # regression parameters. The following code relies
    # on the approach documented in
    #
    #   Heffernan, J M, Smith, R J, & Wahl, L M (2005).
    #     "Perspectives on the basic reproductive ratio."
    #     Journal of the Royal Society Interface, 2(4),
    #     281-293. http://doi.org/10.1098/rsif.2005.0042
    #
    # For the serial interval time, use data reported in
    #
    #   Majumder M S, Cohn E, Fish D & Brownstein J S.
    #     Estimating a feasible serial interval range for
    #     Zika fever [Submitted]. Bull World Health Organ,
    #     E-pub: 9 Feb 2016.
    #     http://dx.doi.org/10.2471/BLT.16.171009
    #
    # Majumder et al report only a range for their
    # estimates without a distribution, so the following
    # code assumes a uniform distribution within the
    # estimated range. (Note that Majumder et al's range
    # is specified in days. It must be converted to weeks
    # to correspond to the observations.)
    #
    # The initial growth rate (r) in the calculation is
    # simply the estimated slope beta[j].
    for (j in 1:max(dept[])) {
        si[j] ~ dunif(10, 23)
        R0[j] <- 1 + beta[j]*si[j]/7
    }

    # Also model an overall R0
    si_mean ~ dunif(10, 23)
    R0_mean <- 1 + beta_mu*si_mean/7
}
