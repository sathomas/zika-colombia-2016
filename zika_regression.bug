# This is a JAGS model for a hierarchical linear regression
# with two levels of hierarchy. The input data consists of
# a set of observations. Each observation includes an
# independent variable (x), a dependent variable (y), and
# a group identification. The model assumes that the
# dependent variable is linearly related to the independent
# variable, and that observations within a group are more
# closely related than observations between groups. In
# particular, it assumes that the slope parameters for
# individual groups are normally distributed around an
# overall slope. Intercept parameters, however, are
# assumed independent for each group and are derived from
# the data.
#
# The model reports goodness of fit using Bayesian p-values.
#
# Observation data is supplied in three vectors:
#    group[] - integer-valued group identity
#    x[]     - independent variable values
#    y[]     - dependent variable values
#
# In addition, a vector of observed intercepts for
# each group is supplied in the `intercept[]` vector.

model {

    # Prior Probabilities
    # -------------------
    # Express all priors as reference (non-informative)
    # priors. The parameters for the model are:
    #
    #   beta_mu     - the mean for the aggregate slope
    #                 (all groups)
    #   beta_sigma  - the standard deviation for the
    #                 aggregate slope (all groups)
    #   sigma       - the standard deviation for the
    #                 residuals (assumed to the same
    #                 for all groups)

    beta_mu     ~ dnorm(0, 0.0001)
    beta_sigma  ~ dunif(0.001, 1000)
    sigma       ~ dunif(0.001, 1000)

    # The Likelihood
    # --------------
    # Model the dependent variable `y` as Normal with mean
    # `mu` and standard deviation `sigma`. Model the mean
    # `mu` as a linear function of the independent variable
    # `x`. The parameters of that linear relationship vary
    # based on the group, and are:
    #
    #   alpha[group[i]]  - intercept for group
    #   beta[group[i]]   - slope for the group
    #
    # The standard deviation `sigma` is assumed to be the
    # same for all observations, regardless of group.
    #
    # Apply the model to all observations.
    for (i in 1:length(y)) {
        y[i]  ~  dnorm(mu[i], 1/(sigma^2))
        mu[i] <- alpha[group[i]] + beta[group[i]]*x[i]
    }

    # Model the slope for each group as Normal with a mean
    # and standard deviation from hyper-priors. Apply the
    # model to all groups.
    for (j in 1:max(group[])) {
        beta[j]  ~ dnorm(beta_mu,  1/(beta_sigma^2))
    }

    # The intercept values for each group are included in
    # the observations. Use the observed value as the
    # mean for a Normal distribution with a standard
    # deviation equal to the overall standard deviation.
    for (j in 1:max(group[])) {
        alpha[j] ~ dnorm(intercept[j], 1/(sigma^2))
    }

    # Posterior Predictive Check
    # --------------------------
    # Calculate the p-value using a sum-of-squares
    # test for fitness. To do that, first calculate
    # the sum of squares of the residuals. (The
    # residuals are the differences between the observed
    # y-values and the values that the current iteration's
    # parameters would predict.)
    for (i in 1:length(y)) {
        sq.res[i] <- pow((y[i] - mu[i]), 2)
    }

    # Next, generate a new y-value given the current
    # iteration's parameters and compare that new
    # value with what the parameters would predict.
    for (i in 1:length(y)) {
        y.new[i]  ~ dnorm(mu.new[i], 1/(sigma^2))
        mu.new[i] <- alpha[group[i]] + beta[group[i]]*x[i]
        sq.new[i] <- pow((y.new[i] - mu[i]),  2)
    }

    # For the goodness of fit test, compare the sums
    # of the two squared values to see which is greater.
    # If the model is a good fit, neither sum is more
    # likely to be greater than the the other. The
    # resulting p-value should, therefore, be close to
    # 0.5.
    test   <- step(sum(sq.new[]) - sum(sq.res[]))
    pvalue <- mean(test)

    # Additional Processing
    # ---------------------
    # Although not part of the regression itself, the
    # model can be used to estimate R0 from the
    # regression parameters. The following code relies
    # on the approach documented in
    #
    #   Heffernan, J M, Smith, R J, & Wahl, L M (2005).
    #     "Perspectives on the basic reproductive ratio."
    #     Journal of the Royal Society Interface, 2(4),
    #     281-293. http://doi.org/10.1098/rsif.2005.0042
    #
    # For the serial interval time, use data reported in
    #
    #   Majumder M S, Cohn E, Fish D & Brownstein J S.
    #     Estimating a feasible serial interval range for
    #     Zika fever [Submitted]. Bull World Health Organ,
    #     E-pub: 9 Feb 2016.
    #     http://dx.doi.org/10.2471/BLT.16.171009
    #
    # Majumder et al report only a range for their
    # estimates without a distribution, so the following
    # code assumes a uniform distribution within the
    # estimated range. (Note that Majumder et al's range
    # is specified in days. It must be converted to weeks
    # to correspond to the observations.)
    #
    # The initial growth rate (r) in the calculation is
    # simply the estimated slope beta[j].
    for (j in 1:max(group[])) {
        si[j] ~ dunif(10, 23)
        R0[j] <- 1 + beta[j]*si[j]/7
    }
}
