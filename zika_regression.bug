# This is a JAGS model for a hierarchical linear regression
# with two levels of hierarchy. The input data consists of
# a set of observations. Each observation includes an
# independent variable (x), a dependent variable (y), and
# a group identification. The model assumes that the
# dependent variable is linearly related to the independent
# variable, and that observations within a group are more
# closely related than observations between groups. In
# particular, it assumes that the slope parameters for
# individual groups are normally distributed around an
# overall slope. Intercept parameters, however, are
# assumed independent for each group and are derived from
# the data.
#
# The model reports goodness of fit using Bayesian p-values.
#
# Observation data is supplied in three vectors:
#    group[] - integer-valued group identity
#    x[]     - independent variable values
#    y[]     - dependent variable values
#
# In addition, a vector of observed intercepts for
# each group is supplied in the `intercept[]` vector.

model {

    # Prior Probabilities
    # -------------------
    # Express all priors as reference (non-informative)
    # priors. The parameters for the model are:
    #
    #   beta_mu     - the mean for the aggregate slope
    #                 (all groups)
    #   beta_sigma  - the standard deviation for the
    #                 aggregate slope (all groups)
    #   sigma       - the standard deviation for the
    #                 residuals (assumed to the same
    #                 for all groups)

    beta_mu     ~ dnorm(0, 0.0001)
    beta_sigma  ~ dunif(0.001, 1000)
    sigma       ~ dunif(0.001, 1000)

    # The Likelihood
    # --------------
    # Model the dependent variable `y` as Normal with mean
    # `mu` and standard deviation `sigma`. Model the mean
    # `mu` as a linear function of the independent variable
    # `x`. The parameters of that linear relationship vary
    # based on the group, and are:
    #
    #   alpha[group[i]]  - intercept for group
    #   beta[group[i]]   - slope for the group
    #
    # The standard deviation `sigma` is assumed to be the
    # same for all observations, regardless of group.
    #
    # Apply the model to all observations.
    for (i in 1:length(y)) {
        y[i]  ~  dnorm(mu[i], 1/(sigma^2))
        mu[i] <- alpha[group[i]] + beta[group[i]]*x[i]
    }

    # Model the slope for each group as Normal with a mean
    # and standard deviation from hyper-priors. Apply the
    # model to all groups.
    for (j in 1:max(group[])) {
        beta[j]  ~ dnorm(beta_mu,  1/(beta_sigma^2))
    }

    # The intercept values for each group are included in
    # the observations. Use the observed value as the
    # mean for a Normal distribution with a standard
    # deviation equal to the overall standard deviation.
    for (j in 1:max(group[])) {
        alpha[j] ~ dnorm(intercept[j], 1/(sigma^2))
    }

    # Posterior Predictive Check
    # --------------------------
    # Calculate the p-value using a sum-of-squares
    # test for fitness. To do that, first calculate
    # the sum of squares of the residuals. (The
    # residuals are the differences between the observed
    # y-values and the values that the current iteration's
    # parameters would predict.)
    for (i in 1:length(y)) {
        sq.res[i] <- pow((y[i] - mu[i]), 2)
    }

    # Next, generate a new y-value given the current
    # iteration's parameters and compare that new
    # value with what the parameters would predict.
    for (i in 1:length(y)) {
        y.new[i]  ~ dnorm(mu.new[i], 1/(sigma^2))
        mu.new[i] <- alpha[group[i]] + beta[group[i]]*x[i]
        sq.new[i] <- pow((y.new[i] - mu[i]),  2)
    }

    # For the goodness of fit test, compare the sums
    # of the two squared values to see which is greater.
    # If the model is a good fit, neither sum is more
    # likely to be greater than the the other. The
    # resulting p-value should, therefore, be close to
    # 0.5.
    test <- step(sum(sq.new[]) - sum(sq.res[]))
    pvalue <- mean(test)

}
